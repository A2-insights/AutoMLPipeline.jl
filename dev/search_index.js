var documenterSearchIndex = {"docs":
[{"location":"man/pipeline/#Pipeline-1","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/learning/#Model-Training-and-Learning-1","page":"Model Training and Learning","title":"Model Training and Learning","text":"","category":"section"},{"location":"lib/typesfunctions/#lib_decisiontree-1","page":"Types and Functions","title":"Types and Functions","text":"","category":"section"},{"location":"lib/typesfunctions/#Index-1","page":"Types and Functions","title":"Index","text":"","category":"section"},{"location":"lib/typesfunctions/#","page":"Types and Functions","title":"Types and Functions","text":"Modules = [\n   AutoMLPipeline,\n   AutoMLPipeline.Pipelines,\n   AutoMLPipeline.BaseFilters,\n   AutoMLPipeline.SKPreprocessors,\n   AutoMLPipeline.DecisionTreeLearners,\n   AutoMLPipeline.Utils,\n   AutoMLPipeline.FeatureSelectors\n]","category":"page"},{"location":"lib/typesfunctions/#Descriptions-1","page":"Types and Functions","title":"Descriptions","text":"","category":"section"},{"location":"lib/typesfunctions/#","page":"Types and Functions","title":"Types and Functions","text":"Modules = [\n   AutoMLPipeline,\n   AutoMLPipeline.Pipelines,\n   AutoMLPipeline.BaseFilters,\n   AutoMLPipeline.SKPreprocessors,\n   AutoMLPipeline.DecisionTreeLearners,\n   AutoMLPipeline.Utils,\n   AutoMLPipeline.FeatureSelectors\n]","category":"page"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.OneHotEncoder","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.OneHotEncoder","text":"OneHotEncoder(Dict(\n   # Nominal columns\n   :nominal_columns => Int[],\n\n   # Nominal column values map. Key is column index, value is list of\n   # possible values for that column.\n   :nominal_column_values_map => Dict{Int,Any}()\n))\n\nTransforms instances with nominal features into one-hot form and coerces the instance matrix to be of element type Float64.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.Imputer","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.Imputer","text":"Imputer(\n   Dict(\n      # Imputation strategy.\n      # Statistic that takes a vector such as mean or median.\n      :strategy => mean\n   )\n)\n\n#Imputes NaN values from Float64 features.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.Wrapper","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.Wrapper","text":"Wrapper(\n   default_args = Dict(\n      # Transformer to call.\n      :transformer => OneHotEncoder(),\n      # Transformer args.\n      :transformer_args => nothing\n   )\n)\n\nWraps around a TSML transformer.\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.BaseFilters.createtransformer","page":"Types and Functions","title":"AutoMLPipeline.BaseFilters.createtransformer","text":"createtransformer(prototype::Transformer, args=nothing)\n\nCreate transformer\n\nprototype: prototype transformer to base new transformer on\noptions: additional options to override prototype's options\n\nReturns: new transformer.\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.DecisionTreeLearners.Adaboost","page":"Types and Functions","title":"AutoMLPipeline.DecisionTreeLearners.Adaboost","text":"Adaboost(\n  Dict(\n    :output => :class,\n    :num_iterations => 7\n  )\n)\n\nAdaboosted decision tree stumps. See DecisionTree.jl's documentation\n\nHyperparameters:\n\n:num_iterations => 7 (number of iterations of AdaBoost)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.DecisionTreeLearners.PrunedTree","page":"Types and Functions","title":"AutoMLPipeline.DecisionTreeLearners.PrunedTree","text":"PrunedTree(\n  Dict(\n    :purity_threshold => 1.0,\n    :max_depth => -1,\n    :min_samples_leaf => 1,\n    :min_samples_split => 2,\n    :min_purity_increase => 0.0\n  )\n)\n\nDecision tree classifier.   See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:purity_threshold => 1.0 (merge leaves having >=thresh combined purity)\n:max_depth => -1 (maximum depth of the decision tree)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.DecisionTreeLearners.RandomForest","page":"Types and Functions","title":"AutoMLPipeline.DecisionTreeLearners.RandomForest","text":"RandomForest(\n  Dict(\n    :output => :class,\n    :num_subfeatures => 0,\n    :num_trees => 10,\n    :partial_sampling => 0.7,\n    :max_depth => -1\n  )\n)\n\nRandom forest classification.  See DecisionTree.jl's documentation\n\nHyperparmeters:\n\n:num_subfeatures => 0  (number of features to consider at random per split)\n:num_trees => 10 (number of trees to train)\n:partial_sampling => 0.7 (fraction of samples to train each tree on)\n:max_depth => -1 (maximum depth of the decision trees)\n:min_samples_leaf => 1 (the minimum number of samples each leaf needs to have)\n:min_samples_split => 2 (the minimum number of samples in needed for a split)\n:min_purity_increase => 0.0 (minimum purity needed for a split)\n\nImplements fit!, transform!\n\n\n\n\n\n","category":"type"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{AutoMLPipeline.DecisionTreeLearners.Adaboost,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(adaboost::Adaboost, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{AutoMLPipeline.DecisionTreeLearners.PrunedTree,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(tree::PrunedTree, features::DataFrame, labels::Vector)\n\nOptimize the hyperparameters of PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.fit!-Tuple{AutoMLPipeline.DecisionTreeLearners.RandomForest,DataFrames.DataFrame,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.fit!","text":"fit!(forest::RandomForest, features::T, labels::Vector) where {T<:Union{Vector,Matrix,DataFrame}}\n\nOptimize the parameters of the RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{AutoMLPipeline.DecisionTreeLearners.Adaboost,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(adaboost::Adaboost, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPredict using the optimized hyperparameters of the trained Adaboost instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{AutoMLPipeline.DecisionTreeLearners.PrunedTree,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(ptree::PrunedTree, features::DataFrame)\n\nPredict using the optimized hyperparameters of the trained PrunedTree instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.AbsTypes.transform!-Tuple{AutoMLPipeline.DecisionTreeLearners.RandomForest,DataFrames.DataFrame}","page":"Types and Functions","title":"AutoMLPipeline.AbsTypes.transform!","text":"transform!(forest::RandomForest, features::T) where {T<:Union{Vector,Matrix,DataFrame}}\n\nPredict using the optimized hyperparameters of the trained RandomForest instance.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.aggregatorclskipmissing-Tuple{Function}","page":"Types and Functions","title":"AutoMLPipeline.Utils.aggregatorclskipmissing","text":"aggregatorclskipmissing(fn::Function)\n\nFunction to create aggregator closure with skipmissing features\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.createmachine","page":"Types and Functions","title":"AutoMLPipeline.Utils.createmachine","text":"createmachine(prototype::Machine, options=nothing)\n\nCreate machine\n\nprototype: prototype machine to base new machine on\noptions: additional options to override prototype's options\n\nReturns: new machine\n\n\n\n\n\n","category":"function"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.holdout-Tuple{Any,Any}","page":"Types and Functions","title":"AutoMLPipeline.Utils.holdout","text":"holdout(n, right_prop)\n\nHoldout method that partitions a collection into two partitions.\n\nn: Size of collection to partition\nright_prop: Percentage of collection placed in right partition\n\nReturns: two partitions of indices, left and right\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.infer_eltype-Tuple{Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.Utils.infer_eltype","text":"infer_eltype(vector::Vector)\n\nReturns element type of vector unless it is Any. If Any, returns the most specific type that can be inferred from the vector elements.\n\nvector: vector to infer element type on\n\nReturns: inferred element type\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.kfold-Tuple{Any,Any}","page":"Types and Functions","title":"AutoMLPipeline.Utils.kfold","text":"kfold(num_instances, num_partitions)\n\nReturns k-fold partitions.\n\nnum_instances: total number of instances\nnum_partitions: number of partitions required\n\nReturns: training set partition.\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.mergedict-Tuple{Dict,Dict}","page":"Types and Functions","title":"AutoMLPipeline.Utils.mergedict","text":"mergedict(first::Dict, second::Dict)\n\nSecond nested dictionary is merged into first.\n\nIf a second dictionary's value as well as the first are both dictionaries, then a merge is conducted between the two inner dictionaries. Otherwise the second's value overrides the first.\n\nfirst: first nested dictionary\nsecond: second nested dictionary\n\nReturns: merged nested dictionary\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.nested_dict_merge-Tuple{Dict,Dict}","page":"Types and Functions","title":"AutoMLPipeline.Utils.nested_dict_merge","text":"nested_dict_merge(first::Dict, second::Dict)\n\nSecond nested dictionary is merged into first.\n\nIf a second dictionary's value as well as the first are both dictionaries, then a merge is conducted between the two inner dictionaries. Otherwise the second's value overrides the first.\n\nfirst: first nested dictionary\nsecond: second nested dictionary\n\nReturns: merged nested dictionary\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.nested_dict_set!-Union{Tuple{T}, Tuple{Dict,Array{T,1},Any}} where T","page":"Types and Functions","title":"AutoMLPipeline.Utils.nested_dict_set!","text":"nested_dict_set!(dict::Dict, keys::Array{T, 1}, value) where {T}\n\nSet value in a nested dictionary.\n\ndict: nested dictionary to assign value\nkeys: keys to access nested dictionaries in sequence\nvalue: value to assign\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.nested_dict_to_tuples-Tuple{Dict}","page":"Types and Functions","title":"AutoMLPipeline.Utils.nested_dict_to_tuples","text":"nested_dict_to_tuples(dict::Dict)\n\nConverts nested dictionary to list of tuples\n\ndict: dictionary that can have other dictionaries as values\n\nReturns: list where elements are ([outer-key, inner-key, ...], value)\n\n\n\n\n\n","category":"method"},{"location":"lib/typesfunctions/#AutoMLPipeline.Utils.score-Tuple{Symbol,Array{T,1} where T,Array{T,1} where T}","page":"Types and Functions","title":"AutoMLPipeline.Utils.score","text":"score(metric::Symbol, actual::Vector, predicted::Vector)\n\nScore learner predictions against ground truth values.\n\nAvailable metrics:\n\n:accuracy\nmetric: metric to assess with\nactual: ground truth values\npredicted: predicted values\n\nReturns: score of learner\n\n\n\n\n\n","category":"method"},{"location":"tutorial/crossvalidation/#Crossvalidation-1","page":"Crossvalidation","title":"Crossvalidation","text":"","category":"section"},{"location":"tutorial/preprocessing/#Preprocessing-1","page":"Preprocessing","title":"Preprocessing","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Let us start by loading the diabetes dataset:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using Random\nENV[\"COLUMNS\"]=1000\nRandom.seed!(123)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using AutoMLPipeline\nusing CSV\ndiabetesdf = CSV.read(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/diabetes.csv\"))\nX = diabetesdf[:,2:end]\nY = diabetesdf[:,1] |> Vector\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"We can check the data by showing the first 5 rows:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"show5(df)=first(df,5); # show first 5 rows\nshow5(diabetesdf)","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"This dataset is a collection diagnostic tests among the  Pima Indians to investigate whether the patient shows  sign of diabetes or not based on certain features:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Number of times pregnant\nPlasma glucose concentration a 2 hours in an oral glucose tolerance test\nDiastolic blood pressure (mm Hg)\nTriceps skin fold thickness (mm)\n2-Hour serum insulin (mu U/ml)\nBody mass index (weight in kg/(height in m)^2)\nDiabetes pedigree function\nAge (years)\nClass variable (0 or 1) indicating diabetec or not","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"What is interesting with this dataset is that one or more numeric columns can be categorical and should be hot-bit encoded. One way to verify is  to compute the number of unique instances for each column and look for  columns with relatively smaller count:","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"[n=>length(unique(x)) for (n,x) in eachcol(diabetesdf,true)] |> collect","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Among the input columns, preg has only 17 unique instances and it can be treated as a categorical variable. However, its description indicates that the feature refers to the number of times the patient is pregnant and can be considered numerical. With this dillema, we need to figure out which representation provides better performance to our classifier. In order to test the two options, we can use the Feature Discriminator module to filter and transform the preg column to either numeric or categorical and choose the pipeline with the optimal performance.","category":"page"},{"location":"tutorial/preprocessing/#CatNumDiscriminator-for-Detecting-Categorical-Numeric-Features-1","page":"Preprocessing","title":"CatNumDiscriminator for Detecting Categorical Numeric Features","text":"","category":"section"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Transform numeric columns with small unique instances to catergories.","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"Let us use CatNumDiscriminator which expects one argument to indicate the maximum number of unique instances in order to consider a particular column as categorical. For the sake of this discussion, let us use its  default value which is 24.","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"using AutoMLPipeline, AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods, AutoMLPipeline.CrossValidators\nusing AutoMLPipeline.DecisionTreeLearners, AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters, AutoMLPipeline.SKPreprocessors\nusing AutoMLPipeline.Utils\n\ndisc = CatNumDiscriminator()\n@pipeline disc\ntr_disc = fit_transform!(disc,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/preprocessing/#","page":"Preprocessing","title":"Preprocessing","text":"show5(tr_disc)","category":"page"},{"location":"tutorial/pipeline/#Pipeline-1","page":"Pipeline","title":"Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"A tutorial for using the @pipeline expression","category":"page"},{"location":"tutorial/pipeline/#Dataset-1","page":"Pipeline","title":"Dataset","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us start the tutorial by loading the dataset.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using Random\nENV[\"COLUMNS\"]=1000\nRandom.seed!(123)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using AutoMLPipeline\nusing CSV\nprofbdata = CSV.read(joinpath(dirname(pathof(AutoMLPipeline)),\"../data/profb.csv\"))\nX = profbdata[:,2:end] \nY = profbdata[:,1] |> Vector\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"We can check the data by showing the first 5 rows:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(df)=first(df,5); # show first 5 rows\nshow5(profbdata)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"This dataset is a collection of pro football scores with the following variables and their descriptions:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Home/Away = Favored team is at home or away\nFavorite Points = Points scored by the favored team\nUnderdog Points = Points scored by the underdog team\nPointspread = Oddsmaker's points to handicap the favored team\nFavorite Name = Code for favored team's name\nUnderdog name = Code for underdog's name\nYear = 89, 90, or 91","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"note: Note\nFor the purpose of this tutorial, we will use the first column, Home vs Away, as the target variable to be predicted using the other columns as input features. For this target output, we are trying to ask whether the model can learn the patterns from its input features to predict whether the game was played at home or away. Since the input features have both categorical and numerical features, the dataset is a good basis to describe  how to extract these two types of features, preprocessed them, and learn the mapping using a one-liner pipeline expression.","category":"page"},{"location":"tutorial/pipeline/#AutoMLPipeline-Modules-and-Instances-1","page":"Pipeline","title":"AutoMLPipeline Modules and Instances","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Before continuing further with the tutorial, let us load the  necessary modules of AutoMLPipeline:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"using AutoMLPipeline, AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods, AutoMLPipeline.CrossValidators\nusing AutoMLPipeline.DecisionTreeLearners, AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters, AutoMLPipeline.SKPreprocessors\nusing AutoMLPipeline.Utils\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us also create some instances of filters, transformers, and models that we can use to preprocess and model the dataset.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"#### Decomposition\npca = SKPreprocessor(\"PCA\"); fa = SKPreprocessor(\"FactorAnalysis\"); \nica = SKPreprocessor(\"FastICA\")\n\n#### Scaler \nrb = SKPreprocessor(\"RobustScaler\"); pt = SKPreprocessor(\"PowerTransformer\") \nnorm = SKPreprocessor(\"Normalizer\"); mx = SKPreprocessor(\"MinMaxScaler\")\n\n#### categorical preprocessing\nohe = OneHotEncoder()\n\n#### Column selector\ndisc = CatNumDiscriminator()\ncatf = CatFeatureSelector(); numf = NumFeatureSelector()\n\n#### Learners\nrf = SKLearner(\"RandomForestClassifier\"); gb = SKLearner(\"GradientBoostingClassifier\")\nlsvc = SKLearner(\"LinearSVC\"); svc = SKLearner(\"SVC\")\nmlp = SKLearner(\"MLPClassifier\"); ada = SKLearner(\"AdaBoostClassifier\")\njrf = RandomForest(); vote = VoteEnsemble(); stack = StackEnsemble()           \nbest = BestLearner()\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#Processing-Categorical-Features-1","page":"Pipeline","title":"Processing Categorical Features","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"For the first illustration, let us extract categorical features of  the data and output some of them using the pipeline expression  and its interface:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_cat = @pipeline catf \ntr_cat = fit_transform!(pop_cat,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_cat)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"One may notice that instead of using fit! and transform,  the example uses fit_transform! instead. The latter is equivalent to calling fit! and transform in sequence which is handy for examining the final output of the transformation prior to  feeding it to the model.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us now transform the caterical features into one-hotbit-encoding (ohe) and examine the results:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_ohe = @pipeline catf |> ohe\ntr_ohe = fit_transform!(pop_ohe,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_ohe)","category":"page"},{"location":"tutorial/pipeline/#Processing-Numerical-Features-1","page":"Pipeline","title":"Processing Numerical Features","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us have an example of extracting the numerical features of the data using different combinations of filters/transformers:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_rb = @pipeline (numf |> rb)\ntr_rb = fit_transform!(pop_rb,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_rb)","category":"page"},{"location":"tutorial/pipeline/#Concatenating-Extracted-Categorical-and-Numerical-Features-1","page":"Pipeline","title":"Concatenating Extracted Categorical and Numerical Features","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"For typical modeling workflow, input features are combinations of categorical features transformer to one-bit encoding together with numerical features normalized or scaled or transformed by decomposition. ","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Here is an example of a typical input feature:","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_com = @pipeline (numf |> norm) + (catf |> ohe)\ntr_com = fit_transform!(pop_com,X,Y)\nnothing #hide","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"show5(tr_com)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"The column size from 6 grew to 60 after the hot-bit encoding was applied because of the large number of unique instances for the categorical columns. ","category":"page"},{"location":"tutorial/pipeline/#Performance-Evaluation-of-the-Pipeline-1","page":"Pipeline","title":"Performance Evaluation of the Pipeline","text":"","category":"section"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"We can add a model at the end of the pipeline and evaluate the performance of the entire pipeline by cross-validation.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"Let us use a linear SVC model and evaluate using 5-fold cross-validation.","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_lsvc = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> lsvc;\ntr_lsvc = crossvalidate(pop_lsvc,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"What about using Gradient Boosting model?","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_gb = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> gb;\ntr_gb = crossvalidate(pop_gb,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"What about using Random Forest model?","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"pop_rf = @pipeline ( (numf |> rb) + (catf |> ohe) + (numf |> pt)) |> jrf;\ntr_rf = crossvalidate(pop_rf,X,Y,\"balanced_accuracy_score\",5)","category":"page"},{"location":"tutorial/pipeline/#","page":"Pipeline","title":"Pipeline","text":"note: Note\nIt can be inferred from the results that linear SVC has the best performance with respect to the different pipelines evaluated. The compact expression supported by the  pipeline makes testing of the different combination of features  and models trivial. It makes performance evaluation   of the pipeline easily manageable in a systematic way.","category":"page"},{"location":"man/ensemble/#Ensembles-1","page":"Ensembles","title":"Ensembles","text":"","category":"section"},{"location":"man/ensemble/#","page":"Ensembles","title":"Ensembles","text":"dfsfs","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Author = \"Paulito P. Palmes\"","category":"page"},{"location":"#AutoMLPipeline-(AMLP)-1","page":"HOME","title":"AutoMLPipeline (AMLP)","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"is a package that makes it trivial to create  complex ML pipeline structures using simple  expressions. AMLP leverages on the built-in macro programming features of Julia to symbolically process, manipulate  pipeline expressions, and automatically discover optimal structures  for machine learning prediction and classification.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"To illustrate, a typical machine learning workflow that extracts numerical features (numf) for ICA (independent component analysis) and  PCA (principal component analysis) transformations, respectively, concatentated with the hot-bit encoding (ohe) of categorical  features (catf) of a given data for RF modeling can be expressed  in AMLP as:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> model = @pipeline (catf |> ohe) + (numf |> pca) + (numf |> ica) |> rf\njulia> fit!(model,Xtrain,Ytrain)\njulia> prediction = transform!(model,Xtest)\njulia> score(:accuracy,prediction,Ytest)\njulia> crossvalidate(model,X,Y,\"accuracy_score\")\njulia> crossvalidate(model,X,Y,\"balanced_accuracy_score\")","category":"page"},{"location":"#Motivations-1","page":"HOME","title":"Motivations","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"The typical workflow in machine learning  classification or prediction requires  some or combination of the following  preprocessing steps together with modeling:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"feature extraction (e.g. ica, pca, svd)\nfeature transformation (e.g. normalization, scaling, ohe)\nfeature selection (anova, correlation)\nmodeling (rf, adaboost, xgboost, lm, svm, mlp)","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Each step has several choices of functions to use together with their corresponding  parameters. Optimizing the performance of the entire pipeline is a combinatorial search of the proper order and combination of preprocessing steps, optimization of their corresponding parameters, together with searching for  the optimal model and its hyper-parameters.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Because of close dependencies among various steps, we can consider the entire process  to be a pipeline optimization problem (POP). POP requires simultaneous optimization of pipeline structure and parameter adaptation of its elements. As a consequence, having an elegant way to express pipeline structure helps in the analysis and implementation of the optimization routines.","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"The target of future work will be the  implementations of different pipeline  optimization algorithms ranging from  evolutionary approaches, integer programming (discrete choices of POP elements),  tree/graph search, and hyper-parameter search.","category":"page"},{"location":"#Package-Features-1","page":"HOME","title":"Package Features","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pipeline API that allows high-level description of processing workflow\nCommon API wrappers for ML libs including Scikitlearn, DecisionTree, etc\nSymbolic pipeline parsing for easy expression  of complexed pipeline structures\nEasily extensible architecture by overloading just two main interfaces: fit! and transform!\nMeta-ensembles that allows composition of    ensembles of ensembles (recursively if needed)    for robust prediction routines\nCategorical and numerical feature selectors for    specialized preprocessing routines based on types","category":"page"},{"location":"#Installation-1","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"AutoMLPipeline is in the Julia Official package registry.  The latest release can be installed at the Julia  prompt using Julia's package management which is triggered by pressing ] at the julia prompt:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> ]\n(v1.0) pkg> add AutoMLPipeline","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> pkg\"add AutoMLPipeline\"","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using Pkg\njulia> Pkg.add(\"AutoMLPipeline\")","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Once AutoMLPipeline is installed, you can  load it by:","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> using AutoMLPipeline","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"or ","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"julia> import AutoMLPipeline","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"Generally, you will need the different learners/transformers and utils in AMLP for to carry-out the processing and modeling routines. ","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"using AutoMLPipeline \nusing AutoMLPipeline.FeatureSelectors\nusing AutoMLPipeline.EnsembleMethods\nusing AutoMLPipeline.CrossValidators \nusing AutoMLPipeline.DecisionTreeLearners\nusing AutoMLPipeline.Pipelines\nusing AutoMLPipeline.BaseFilters\nusing AutoMLPipeline.SKPreprocessors \nusing AutoMLPipeline.Utils`","category":"page"},{"location":"#Tutorial-Outline-1","page":"HOME","title":"Tutorial Outline","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"tutorial/pipeline.md\",\n  \"tutorial/preprocessing.md\",\n  \"tutorial/learning.md\",\n  \"tutorial/crossvalidation.md\"\n]\nDepth = 3","category":"page"},{"location":"#Manual-Outline-1","page":"HOME","title":"Manual Outline","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"man/pipeline.md\",\n  \"man/ensemble.md\",\n  \"man/learners.md\",\n  \"man/preprocessing.md\"\n]\nDepth = 3","category":"page"},{"location":"#ML-Library-1","page":"HOME","title":"ML Library","text":"","category":"section"},{"location":"#","page":"HOME","title":"HOME","text":"Pages = [\n  \"lib/typesfunctions.md\"\n]","category":"page"},{"location":"#","page":"HOME","title":"HOME","text":"","category":"page"},{"location":"man/learners/#Learners-1","page":"Learners","title":"Learners","text":"","category":"section"},{"location":"man/preprocessing/#Ensembles-1","page":"Preprocessing","title":"Ensembles","text":"","category":"section"}]
}
